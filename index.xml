<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Shuvo Islam Khan</title>
    <link>https://shuvoislamkhan.github.io/</link>
    <description>Recent content on Shuvo Islam Khan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://shuvoislamkhan.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Reward Hacking in Reinforcement Learning</title>
      <link>https://shuvoislamkhan.github.io/posts/2024-11-28-reward-hacking/</link>
      <pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>https://shuvoislamkhan.github.io/posts/2024-11-28-reward-hacking/</guid>
      <description>&lt;p&gt;Reward hacking occurs when a &lt;a href=&#34;(https://shuvoislamkhan.github.io/posts/2018-02-19-rl-overview/)&#34;&gt;reinforcement learning (RL)&lt;/a&gt; agent &lt;a href=&#34;https://shuvoislamkhan.github.io/posts/2018-01-23-multi-armed-bandit/#exploitation-vs-exploration&#34;&gt;exploits&lt;/a&gt; flaws or ambiguities in the reward function to achieve high rewards, without genuinely learning or completing the intended task. Reward hacking exists because RL environments are often imperfect, and it is fundamentally challenging to accurately specify a reward function.&lt;/p&gt;
&lt;p&gt;With the rise of &lt;a href=&#34;https://shuvoislamkhan.github.io/posts/2019-01-31-lm/&#34;&gt;language models&lt;/a&gt; generalizing to a broad spectrum of tasks and RLHF becomes a de facto method for alignment training, reward hacking in RL training of language models has become a critical practical challenge. Instances where the model learns to modify unit tests to pass coding tasks, or where responses contain biases that mimic a user&amp;rsquo;s preference, are pretty concerning and are likely one of the major blockers for real-world deployment of more autonomous use cases of AI models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Contrastive Representation Learning</title>
      <link>https://shuvoislamkhan.github.io/posts/2021-05-31-contrastive/</link>
      <pubDate>Mon, 31 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://shuvoislamkhan.github.io/posts/2021-05-31-contrastive/</guid>
      <description>&lt;!-- The main idea of contrastive learning is to learn representations such that similar samples stay close to each other, while dissimilar ones are far apart. Contrastive learning can be applied to both supervised and unsupervised data and has been shown to achieve good performance on a variety of vision and language tasks. --&gt;
&lt;p&gt;The goal of contrastive representation learning is to learn such an embedding space in which similar sample pairs stay close to each other while dissimilar ones are far apart. Contrastive learning can be applied to both supervised and unsupervised settings. When working with unsupervised data, contrastive learning is one of the most powerful approaches in &lt;a href=&#34;https://shuvoislamkhan.github.io/posts/2019-11-10-self-supervised/&#34;&gt;self-supervised learning&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
  
    
   
    
    <item>
      <title>about</title>
      <link>https://shuvoislamkhan.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://shuvoislamkhan.github.io/about/</guid>
      <description></description>
    </item>
    
    
  </channel>
</rss>
